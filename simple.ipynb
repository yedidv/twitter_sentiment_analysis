{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "212983865df447a587cd02ed42187f39760d467fb0227a7a0f46f8f8baf099d2"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(str1, str2): \n",
    "    ## Jaccard score function, given by kaggle competition main page \n",
    "\n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split()) \n",
    "    c = a.intersection(b) \n",
    "    return float(len(c)) / (len(a) + len(b) - len(c)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## Read in the data files\n",
    "train = pd.read_csv('train.csv') \n",
    "test = pd.read_csv('test.csv') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the null values \n",
    "train = train.dropna() \n",
    "\n",
    "## Divide into sentiment dataframes\n",
    "positive_train = train[train.sentiment == 'positive'] \n",
    "negative_train = train[train.sentiment == 'negative'] \n",
    "neutral_train = train[train.sentiment == 'neutral'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We are going to use sklearn countvectorize \n",
    "\n",
    "cv = CountVectorizer(\n",
    "    ## The max_df and min_df functions remove words that occur too often or not often enough. Anything above the max_df threshold is probably a filler word like 'an' or 'the', etc. Anything below the word count of min_df is most likely a typo, and is not needed for the dataset. We will later assign words that fall outside these thresholds with a value of 0 \n",
    "    max_df = 0.95, min_df = 2, max_features = 10000, stop_words = 'english'\n",
    "    ) \n",
    "\n",
    "## Creates the total dictionary of words used in the tweets \n",
    "train_cv = cv.fit_transform(train.text) \n",
    "\n",
    "## Converts document to see word distribution for all the words \n",
    "positive = cv.transform(positive_train.text) \n",
    "negative = cv.transform(negative_train.text) \n",
    "neutral = cv.transform(neutral_train.text) \n",
    "\n",
    "## Word distribution in a dataframe\n",
    "positive_df = pd.DataFrame(positive.toarray(), columns = cv.get_feature_names()) \n",
    "negative_df = pd.DataFrame(negative.toarray(), columns = cv.get_feature_names()) \n",
    "neutral_df = pd.DataFrame(neutral.toarray(), columns = cv.get_feature_names()) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "positive_words = {} \n",
    "negative_words = {} \n",
    "neutral_words = {} \n",
    "\n",
    "## We want to see each word and see how often it appears in the tweet sentiment \n",
    "for feature in cv.get_feature_names(): ## Total dictionary \n",
    "\n",
    "    ## Find how often each word appears in the dataframe, divide it by the total number of words to get the weight proportion\n",
    "    positive_words[feature] = positive_df[feature].sum() / positive_train.shape[0] \n",
    "    negative_words[feature] = negative_df[feature].sum() / negative_train.shape[0] \n",
    "    neutral_words[feature] = neutral_df[feature].sum() / neutral_train.shape[0]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To further distinguish the words in the tweets from each other, the author of this method proposed that we subtract the portion the word appears in its own sentiment from the portion of the time it appears in other sentiments \n",
    "\n",
    "positive_words_adjusted = {} \n",
    "negative_words_adjusted = {} \n",
    "neutral_words_adjusted = {} \n",
    "\n",
    "for key, value in negative_words.items(): \n",
    "    negative_words_adjusted[key] = negative_words[key] - (neutral_words[key] + positive_words[key] ) \n",
    "for key, value in positive_words.items(): \n",
    "    positive_words_adjusted[key] = positive_words[key] - (neutral_words[key] + negative_words[key]) \n",
    "for key, value in neutral_words.items(): \n",
    "    neutral_words_adjusted[key] = neutral_words[key] - (negative_words[key] + positive_words[key]) \n",
    "\n",
    "## This concludes the end of pre-processing the weights of the individual words in the tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def Prediction(text, sentiment): \n",
    "    neutral_text = text\n",
    "    text = text.lower().split() \n",
    "    #print(text)\n",
    "    if sentiment == 'neutral' or len(text) <= 3: \n",
    "        return neutral_text \n",
    "        \n",
    "    elif sentiment == 'positive': \n",
    "        dictionary = positive_words_adjusted \n",
    "    elif sentiment == 'negative': \n",
    "        dictionary = negative_words_adjusted \n",
    "    \n",
    "\n",
    "    total_weight = {}\n",
    "    \n",
    "    ## Find all the possible start locations for the string\n",
    "    for start in range(len(text)): \n",
    "        \n",
    "        ## Find all the possible end locations for the string \n",
    "        for end in range(start, len(text)): \n",
    "            ## Returns a list of the possible phrase \n",
    "            subset = (text[start:end + 1])\n",
    "             \n",
    "            b = 0\n",
    "            for word in subset: \n",
    "                try: \n",
    "                    b = b + dictionary[word] \n",
    "                   \n",
    "                     \n",
    "                except KeyError: \n",
    "                    b = b + 0\n",
    "            total_weight[b] = subset \n",
    "    \n",
    "    final_weight = {} \n",
    "    weights = -1000\n",
    "    largest_substring = []\n",
    "    for key, value in total_weight.items(): \n",
    "        if key > weights: \n",
    "            weights = key \n",
    "            largest_substring = value \n",
    "    return ' '.join(largest_substring)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = [] \n",
    "\n",
    "for index, rows in train.iterrows(): \n",
    "    output.append(Prediction(rows.text, rows.sentiment))\n",
    "\n",
    "train['output'] = output\n",
    "\n",
    "## The first output column wasn't flattened while the rest were. So it was quickly fixed here\n",
    "train.output.iloc[1] = ' '.join(train.output.iloc[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head() \n",
    "## Calculate the Jaccard score for the training data \n",
    "jaccard = []\n",
    "for index, row in train.iterrows(): \n",
    "    jaccard.append(Jaccard(row.selected_text, row.output))\n",
    "train['jaccard'] = jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.5841617068218866\n"
     ]
    }
   ],
   "source": [
    "print(train.jaccard.mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Calculate the Output for the test data \n",
    "\n",
    "output = [] \n",
    "for index, rows in test.iterrows(): \n",
    "    output.append(Prediction(rows.text, rows.sentiment))\n",
    "\n",
    "test['selected_text'] = output \n",
    "\n",
    "submission = test[['textID', 'selected_text']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('Simple_Submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}